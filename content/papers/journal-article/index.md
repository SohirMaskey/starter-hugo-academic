---
abstract: Message passing neural networks (MPNN) have seen a steep rise in
  popularity since their introduction as generalizations of convolutional neural
  networks to graph-structured data, and are now considered state-of-the-art
  tools for solving a large variety of graph-focused problems. We study the
  generalization error of MPNNs in graph classification and regression. We
  assume that graphs of different classes are sampled from different random
  graph models. We show that, when training a MPNN on a dataset sampled from
  such a distribution, the generalization gap increases in the complexity of the
  MPNN, and decreases, not only with respect to the number of training samples,
  but also with the average number of nodes in the graphs. This shows how a MPNN
  with high complexity can generalize from a small dataset of graphs, as long as
  the graphs are large. The generalization bound is derived from a uniform
  convergence result, that shows that any MPNN, applied on a graph, approximates
  the MPNN applied on the geometric model that the graph discretizes.
slides: example
url_pdf: http://arxiv.org/pdf/1512.04133v1
publication_types:
  - "1"
authors:
  - admin
  - Ron Levie
  - Yunseok Lee
  - Gitta Kutyniok
summary: "We leverage the infinite-node limit of graph message passing neural networks to derive a uniform generalization bound that decreases with respect to the graphs' sizes."
url_dataset: ""
url_project: ""
author_notes:
  - Equal contribution
  - Equal contribution
publication_short: NeurIPS 2022
url_source: ""
url_video: ""
publication: Advances in Neural Information Processing Systems 2022
featured: false
date: 2022-10-01T00:00:00Z
url_slides: ""
title: "Generalization Analysis of Message Passing Neural Networks on Large
  Random Graphs"
tags:
  - Source Themes
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
publishDate: 2022-10-01T00:00:00Z
url_poster: ""
url_code: ""
doi: ""
---
